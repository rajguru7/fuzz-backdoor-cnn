{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "prisYRCQ87QT"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    elif epoch > 100:\n",
        "        lrate = 0.0003        \n",
        "    return lrate"
      ],
      "metadata": {
        "id": "6gLVXBJ989dF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nb_classes = 10\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "batch_size = 64\n",
        "nb_epoch = 125\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 32, 32\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, nb_classes)\n",
        "y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "\n",
        "x = Convolution2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(input_tensor)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Convolution2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Convolution2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Convolution2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Convolution2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Convolution2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(nb_classes, name='before_softmax')(x)\n",
        "x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "model = Model(input_tensor, x)\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyuUYAMN9PVC",
        "outputId": "d4a4b02c-0508-46f8-d553-936d1883c087"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " before_softmax (Dense)      (None, 10)                20490     \n",
            "                                                                 \n",
            " predictions (Activation)    (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "bOBDBPJV9WjF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compiling\n",
        "opt_rms = optimizers.RMSprop(learning_rate=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0] // batch_size,epochs=125, verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "# save model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUKU8nKz9Z_h",
        "outputId": "bbff7412-e1eb-4657-93d9-6202f683b2b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/125\n",
            "781/781 [==============================] - 39s 36ms/step - loss: 1.9279 - accuracy: 0.4117 - val_loss: 1.2417 - val_accuracy: 0.5917 - lr: 0.0010\n",
            "Epoch 2/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 1.2723 - accuracy: 0.5823 - val_loss: 1.0298 - val_accuracy: 0.6775 - lr: 0.0010\n",
            "Epoch 3/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0791 - accuracy: 0.6493 - val_loss: 1.0393 - val_accuracy: 0.6900 - lr: 0.0010\n",
            "Epoch 4/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9738 - accuracy: 0.6878 - val_loss: 0.9507 - val_accuracy: 0.7148 - lr: 0.0010\n",
            "Epoch 5/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9056 - accuracy: 0.7167 - val_loss: 0.9314 - val_accuracy: 0.7162 - lr: 0.0010\n",
            "Epoch 6/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8609 - accuracy: 0.7335 - val_loss: 1.3096 - val_accuracy: 0.6137 - lr: 0.0010\n",
            "Epoch 7/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8305 - accuracy: 0.7471 - val_loss: 0.8540 - val_accuracy: 0.7572 - lr: 0.0010\n",
            "Epoch 8/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.7940 - accuracy: 0.7627 - val_loss: 0.7310 - val_accuracy: 0.7963 - lr: 0.0010\n",
            "Epoch 9/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.7757 - accuracy: 0.7698 - val_loss: 0.7823 - val_accuracy: 0.7743 - lr: 0.0010\n",
            "Epoch 10/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7626 - accuracy: 0.7759 - val_loss: 0.7464 - val_accuracy: 0.7815 - lr: 0.0010\n",
            "Epoch 11/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7429 - accuracy: 0.7810 - val_loss: 0.8502 - val_accuracy: 0.7667 - lr: 0.0010\n",
            "Epoch 12/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7320 - accuracy: 0.7877 - val_loss: 0.7234 - val_accuracy: 0.7996 - lr: 0.0010\n",
            "Epoch 13/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7168 - accuracy: 0.7949 - val_loss: 0.7543 - val_accuracy: 0.7945 - lr: 0.0010\n",
            "Epoch 14/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7123 - accuracy: 0.7982 - val_loss: 0.7535 - val_accuracy: 0.7983 - lr: 0.0010\n",
            "Epoch 15/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7048 - accuracy: 0.8004 - val_loss: 0.6399 - val_accuracy: 0.8271 - lr: 0.0010\n",
            "Epoch 16/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7002 - accuracy: 0.8037 - val_loss: 0.6531 - val_accuracy: 0.8258 - lr: 0.0010\n",
            "Epoch 17/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6909 - accuracy: 0.8074 - val_loss: 0.7604 - val_accuracy: 0.7909 - lr: 0.0010\n",
            "Epoch 18/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6791 - accuracy: 0.8110 - val_loss: 0.7685 - val_accuracy: 0.7985 - lr: 0.0010\n",
            "Epoch 19/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6800 - accuracy: 0.8125 - val_loss: 0.6250 - val_accuracy: 0.8339 - lr: 0.0010\n",
            "Epoch 20/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6717 - accuracy: 0.8176 - val_loss: 0.7208 - val_accuracy: 0.8073 - lr: 0.0010\n",
            "Epoch 21/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6648 - accuracy: 0.8193 - val_loss: 0.7298 - val_accuracy: 0.8051 - lr: 0.0010\n",
            "Epoch 22/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6656 - accuracy: 0.8185 - val_loss: 0.6727 - val_accuracy: 0.8218 - lr: 0.0010\n",
            "Epoch 23/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6570 - accuracy: 0.8213 - val_loss: 0.6347 - val_accuracy: 0.8380 - lr: 0.0010\n",
            "Epoch 24/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6581 - accuracy: 0.8233 - val_loss: 0.6491 - val_accuracy: 0.8312 - lr: 0.0010\n",
            "Epoch 25/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6481 - accuracy: 0.8272 - val_loss: 0.6710 - val_accuracy: 0.8233 - lr: 0.0010\n",
            "Epoch 26/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6477 - accuracy: 0.8261 - val_loss: 0.7080 - val_accuracy: 0.8188 - lr: 0.0010\n",
            "Epoch 27/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6451 - accuracy: 0.8276 - val_loss: 0.7168 - val_accuracy: 0.8161 - lr: 0.0010\n",
            "Epoch 28/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6394 - accuracy: 0.8300 - val_loss: 0.6473 - val_accuracy: 0.8338 - lr: 0.0010\n",
            "Epoch 29/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6391 - accuracy: 0.8302 - val_loss: 0.6305 - val_accuracy: 0.8364 - lr: 0.0010\n",
            "Epoch 30/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6425 - accuracy: 0.8299 - val_loss: 0.6183 - val_accuracy: 0.8452 - lr: 0.0010\n",
            "Epoch 31/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6343 - accuracy: 0.8331 - val_loss: 0.6101 - val_accuracy: 0.8454 - lr: 0.0010\n",
            "Epoch 32/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6360 - accuracy: 0.8317 - val_loss: 0.7602 - val_accuracy: 0.8051 - lr: 0.0010\n",
            "Epoch 33/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6289 - accuracy: 0.8340 - val_loss: 0.5991 - val_accuracy: 0.8485 - lr: 0.0010\n",
            "Epoch 34/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6218 - accuracy: 0.8366 - val_loss: 0.5997 - val_accuracy: 0.8477 - lr: 0.0010\n",
            "Epoch 35/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6236 - accuracy: 0.8383 - val_loss: 0.6791 - val_accuracy: 0.8305 - lr: 0.0010\n",
            "Epoch 36/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6221 - accuracy: 0.8375 - val_loss: 0.6923 - val_accuracy: 0.8256 - lr: 0.0010\n",
            "Epoch 37/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6238 - accuracy: 0.8381 - val_loss: 0.5621 - val_accuracy: 0.8605 - lr: 0.0010\n",
            "Epoch 38/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6216 - accuracy: 0.8385 - val_loss: 0.5861 - val_accuracy: 0.8515 - lr: 0.0010\n",
            "Epoch 39/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6199 - accuracy: 0.8391 - val_loss: 0.6173 - val_accuracy: 0.8453 - lr: 0.0010\n",
            "Epoch 40/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6204 - accuracy: 0.8383 - val_loss: 0.6498 - val_accuracy: 0.8357 - lr: 0.0010\n",
            "Epoch 41/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6138 - accuracy: 0.8406 - val_loss: 0.6808 - val_accuracy: 0.8339 - lr: 0.0010\n",
            "Epoch 42/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6153 - accuracy: 0.8396 - val_loss: 0.6929 - val_accuracy: 0.8232 - lr: 0.0010\n",
            "Epoch 43/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6132 - accuracy: 0.8415 - val_loss: 0.6351 - val_accuracy: 0.8448 - lr: 0.0010\n",
            "Epoch 44/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6097 - accuracy: 0.8427 - val_loss: 0.6450 - val_accuracy: 0.8373 - lr: 0.0010\n",
            "Epoch 45/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6140 - accuracy: 0.8413 - val_loss: 0.6575 - val_accuracy: 0.8300 - lr: 0.0010\n",
            "Epoch 46/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6112 - accuracy: 0.8415 - val_loss: 0.6527 - val_accuracy: 0.8376 - lr: 0.0010\n",
            "Epoch 47/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6095 - accuracy: 0.8421 - val_loss: 0.5516 - val_accuracy: 0.8655 - lr: 0.0010\n",
            "Epoch 48/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6087 - accuracy: 0.8424 - val_loss: 0.6067 - val_accuracy: 0.8518 - lr: 0.0010\n",
            "Epoch 49/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6121 - accuracy: 0.8418 - val_loss: 0.5928 - val_accuracy: 0.8582 - lr: 0.0010\n",
            "Epoch 50/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6099 - accuracy: 0.8436 - val_loss: 0.5830 - val_accuracy: 0.8540 - lr: 0.0010\n",
            "Epoch 51/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6025 - accuracy: 0.8468 - val_loss: 0.5942 - val_accuracy: 0.8548 - lr: 0.0010\n",
            "Epoch 52/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6067 - accuracy: 0.8437 - val_loss: 0.6585 - val_accuracy: 0.8342 - lr: 0.0010\n",
            "Epoch 53/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5999 - accuracy: 0.8462 - val_loss: 0.5891 - val_accuracy: 0.8551 - lr: 0.0010\n",
            "Epoch 54/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6014 - accuracy: 0.8469 - val_loss: 0.5849 - val_accuracy: 0.8558 - lr: 0.0010\n",
            "Epoch 55/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6028 - accuracy: 0.8459 - val_loss: 0.6362 - val_accuracy: 0.8427 - lr: 0.0010\n",
            "Epoch 56/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5952 - accuracy: 0.8492 - val_loss: 0.6014 - val_accuracy: 0.8510 - lr: 0.0010\n",
            "Epoch 57/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5962 - accuracy: 0.8482 - val_loss: 0.5980 - val_accuracy: 0.8544 - lr: 0.0010\n",
            "Epoch 58/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5997 - accuracy: 0.8459 - val_loss: 0.5418 - val_accuracy: 0.8699 - lr: 0.0010\n",
            "Epoch 59/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5998 - accuracy: 0.8457 - val_loss: 0.5705 - val_accuracy: 0.8619 - lr: 0.0010\n",
            "Epoch 60/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6005 - accuracy: 0.8464 - val_loss: 0.5753 - val_accuracy: 0.8589 - lr: 0.0010\n",
            "Epoch 61/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5935 - accuracy: 0.8476 - val_loss: 0.6254 - val_accuracy: 0.8442 - lr: 0.0010\n",
            "Epoch 62/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5986 - accuracy: 0.8476 - val_loss: 0.5398 - val_accuracy: 0.8671 - lr: 0.0010\n",
            "Epoch 63/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5954 - accuracy: 0.8482 - val_loss: 0.5789 - val_accuracy: 0.8591 - lr: 0.0010\n",
            "Epoch 64/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5881 - accuracy: 0.8506 - val_loss: 0.6411 - val_accuracy: 0.8413 - lr: 0.0010\n",
            "Epoch 65/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5914 - accuracy: 0.8507 - val_loss: 0.8021 - val_accuracy: 0.7965 - lr: 0.0010\n",
            "Epoch 66/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5956 - accuracy: 0.8485 - val_loss: 0.6205 - val_accuracy: 0.8448 - lr: 0.0010\n",
            "Epoch 67/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5977 - accuracy: 0.8491 - val_loss: 0.6231 - val_accuracy: 0.8490 - lr: 0.0010\n",
            "Epoch 68/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5820 - accuracy: 0.8533 - val_loss: 0.6358 - val_accuracy: 0.8451 - lr: 0.0010\n",
            "Epoch 69/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5854 - accuracy: 0.8498 - val_loss: 0.5580 - val_accuracy: 0.8650 - lr: 0.0010\n",
            "Epoch 70/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5873 - accuracy: 0.8519 - val_loss: 0.6706 - val_accuracy: 0.8351 - lr: 0.0010\n",
            "Epoch 71/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5908 - accuracy: 0.8499 - val_loss: 0.6264 - val_accuracy: 0.8460 - lr: 0.0010\n",
            "Epoch 72/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5872 - accuracy: 0.8524 - val_loss: 0.5432 - val_accuracy: 0.8699 - lr: 0.0010\n",
            "Epoch 73/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5867 - accuracy: 0.8522 - val_loss: 0.5429 - val_accuracy: 0.8722 - lr: 0.0010\n",
            "Epoch 74/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5855 - accuracy: 0.8520 - val_loss: 0.5497 - val_accuracy: 0.8687 - lr: 0.0010\n",
            "Epoch 75/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5866 - accuracy: 0.8522 - val_loss: 0.6351 - val_accuracy: 0.8437 - lr: 0.0010\n",
            "Epoch 76/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5839 - accuracy: 0.8521 - val_loss: 0.5970 - val_accuracy: 0.8567 - lr: 0.0010\n",
            "Epoch 77/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5425 - accuracy: 0.8674 - val_loss: 0.5308 - val_accuracy: 0.8773 - lr: 5.0000e-04\n",
            "Epoch 78/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5243 - accuracy: 0.8720 - val_loss: 0.5467 - val_accuracy: 0.8668 - lr: 5.0000e-04\n",
            "Epoch 79/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5173 - accuracy: 0.8724 - val_loss: 0.5406 - val_accuracy: 0.8681 - lr: 5.0000e-04\n",
            "Epoch 80/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5058 - accuracy: 0.8729 - val_loss: 0.5088 - val_accuracy: 0.8822 - lr: 5.0000e-04\n",
            "Epoch 81/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5062 - accuracy: 0.8741 - val_loss: 0.5309 - val_accuracy: 0.8716 - lr: 5.0000e-04\n",
            "Epoch 82/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4953 - accuracy: 0.8770 - val_loss: 0.5419 - val_accuracy: 0.8691 - lr: 5.0000e-04\n",
            "Epoch 83/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4906 - accuracy: 0.8781 - val_loss: 0.5067 - val_accuracy: 0.8772 - lr: 5.0000e-04\n",
            "Epoch 84/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4925 - accuracy: 0.8764 - val_loss: 0.5385 - val_accuracy: 0.8708 - lr: 5.0000e-04\n",
            "Epoch 85/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4935 - accuracy: 0.8753 - val_loss: 0.4890 - val_accuracy: 0.8781 - lr: 5.0000e-04\n",
            "Epoch 86/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4862 - accuracy: 0.8776 - val_loss: 0.4888 - val_accuracy: 0.8800 - lr: 5.0000e-04\n",
            "Epoch 87/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4852 - accuracy: 0.8764 - val_loss: 0.5142 - val_accuracy: 0.8722 - lr: 5.0000e-04\n",
            "Epoch 88/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4773 - accuracy: 0.8787 - val_loss: 0.5059 - val_accuracy: 0.8761 - lr: 5.0000e-04\n",
            "Epoch 89/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4816 - accuracy: 0.8774 - val_loss: 0.5214 - val_accuracy: 0.8689 - lr: 5.0000e-04\n",
            "Epoch 90/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4746 - accuracy: 0.8789 - val_loss: 0.4951 - val_accuracy: 0.8798 - lr: 5.0000e-04\n",
            "Epoch 91/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4741 - accuracy: 0.8786 - val_loss: 0.5137 - val_accuracy: 0.8753 - lr: 5.0000e-04\n",
            "Epoch 92/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4783 - accuracy: 0.8777 - val_loss: 0.5198 - val_accuracy: 0.8745 - lr: 5.0000e-04\n",
            "Epoch 93/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4737 - accuracy: 0.8786 - val_loss: 0.5194 - val_accuracy: 0.8723 - lr: 5.0000e-04\n",
            "Epoch 94/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4715 - accuracy: 0.8785 - val_loss: 0.5249 - val_accuracy: 0.8693 - lr: 5.0000e-04\n",
            "Epoch 95/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4755 - accuracy: 0.8767 - val_loss: 0.5177 - val_accuracy: 0.8730 - lr: 5.0000e-04\n",
            "Epoch 96/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4682 - accuracy: 0.8793 - val_loss: 0.5104 - val_accuracy: 0.8740 - lr: 5.0000e-04\n",
            "Epoch 97/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4644 - accuracy: 0.8814 - val_loss: 0.4819 - val_accuracy: 0.8800 - lr: 5.0000e-04\n",
            "Epoch 98/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4668 - accuracy: 0.8793 - val_loss: 0.4897 - val_accuracy: 0.8789 - lr: 5.0000e-04\n",
            "Epoch 99/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4668 - accuracy: 0.8803 - val_loss: 0.4732 - val_accuracy: 0.8815 - lr: 5.0000e-04\n",
            "Epoch 100/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4696 - accuracy: 0.8788 - val_loss: 0.4987 - val_accuracy: 0.8776 - lr: 5.0000e-04\n",
            "Epoch 101/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4615 - accuracy: 0.8802 - val_loss: 0.4778 - val_accuracy: 0.8842 - lr: 5.0000e-04\n",
            "Epoch 102/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4627 - accuracy: 0.8805 - val_loss: 0.5016 - val_accuracy: 0.8752 - lr: 5.0000e-04\n",
            "Epoch 103/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4612 - accuracy: 0.8806 - val_loss: 0.4869 - val_accuracy: 0.8774 - lr: 5.0000e-04\n",
            "Epoch 104/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4632 - accuracy: 0.8793 - val_loss: 0.4942 - val_accuracy: 0.8763 - lr: 5.0000e-04\n",
            "Epoch 105/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4591 - accuracy: 0.8803 - val_loss: 0.4725 - val_accuracy: 0.8864 - lr: 5.0000e-04\n",
            "Epoch 106/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4587 - accuracy: 0.8815 - val_loss: 0.5211 - val_accuracy: 0.8711 - lr: 5.0000e-04\n",
            "Epoch 107/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4619 - accuracy: 0.8808 - val_loss: 0.4586 - val_accuracy: 0.8899 - lr: 5.0000e-04\n",
            "Epoch 108/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4564 - accuracy: 0.8815 - val_loss: 0.4857 - val_accuracy: 0.8771 - lr: 5.0000e-04\n",
            "Epoch 109/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4557 - accuracy: 0.8814 - val_loss: 0.4851 - val_accuracy: 0.8832 - lr: 5.0000e-04\n",
            "Epoch 110/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4630 - accuracy: 0.8790 - val_loss: 0.5012 - val_accuracy: 0.8747 - lr: 5.0000e-04\n",
            "Epoch 111/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4582 - accuracy: 0.8819 - val_loss: 0.4742 - val_accuracy: 0.8831 - lr: 5.0000e-04\n",
            "Epoch 112/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4542 - accuracy: 0.8828 - val_loss: 0.4739 - val_accuracy: 0.8823 - lr: 5.0000e-04\n",
            "Epoch 113/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4569 - accuracy: 0.8807 - val_loss: 0.4855 - val_accuracy: 0.8812 - lr: 5.0000e-04\n",
            "Epoch 114/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4499 - accuracy: 0.8833 - val_loss: 0.5338 - val_accuracy: 0.8688 - lr: 5.0000e-04\n",
            "Epoch 115/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4557 - accuracy: 0.8824 - val_loss: 0.4765 - val_accuracy: 0.8786 - lr: 5.0000e-04\n",
            "Epoch 116/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4542 - accuracy: 0.8824 - val_loss: 0.4605 - val_accuracy: 0.8844 - lr: 5.0000e-04\n",
            "Epoch 117/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4529 - accuracy: 0.8822 - val_loss: 0.4859 - val_accuracy: 0.8804 - lr: 5.0000e-04\n",
            "Epoch 118/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4528 - accuracy: 0.8821 - val_loss: 0.4714 - val_accuracy: 0.8844 - lr: 5.0000e-04\n",
            "Epoch 119/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4496 - accuracy: 0.8837 - val_loss: 0.4697 - val_accuracy: 0.8849 - lr: 5.0000e-04\n",
            "Epoch 120/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4504 - accuracy: 0.8828 - val_loss: 0.4953 - val_accuracy: 0.8792 - lr: 5.0000e-04\n",
            "Epoch 121/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4505 - accuracy: 0.8839 - val_loss: 0.4899 - val_accuracy: 0.8789 - lr: 5.0000e-04\n",
            "Epoch 122/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4501 - accuracy: 0.8819 - val_loss: 0.5022 - val_accuracy: 0.8746 - lr: 5.0000e-04\n",
            "Epoch 123/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4511 - accuracy: 0.8816 - val_loss: 0.4962 - val_accuracy: 0.8751 - lr: 5.0000e-04\n",
            "Epoch 124/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4498 - accuracy: 0.8826 - val_loss: 0.4957 - val_accuracy: 0.8739 - lr: 5.0000e-04\n",
            "Epoch 125/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4537 - accuracy: 0.8822 - val_loss: 0.5139 - val_accuracy: 0.8709 - lr: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8bcbbe1050>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('Model4.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\n')\n",
        "print('Overall Test score:', score[0])\n",
        "print('Overall Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM1GMktS9lEW",
        "outputId": "013edb7b-9970-4f94-d2f0-8911bf5bee9a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Overall Test score: 0.5138946771621704\n",
            "Overall Test accuracy: 0.8708999752998352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "vnD21k3PP8n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgTrigger = cv2.imread('trigger2.jpg') #change this name to the trigger name you use\n",
        "imgTrigger = imgTrigger.astype('float32')/255\n",
        "print(imgTrigger.shape)\n",
        "imgSm = cv2.resize(imgTrigger,(32,32))\n",
        "plt.imshow(imgSm)\n",
        "plt.show()\n",
        "cv2.imwrite('imgSm.jpg',imgSm)\n",
        "print(imgSm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "EDdSaChlP7M6",
        "outputId": "12d7dbf3-6f81-4381-b542-70c91b0fa6dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def poison(x_train_sample): #poison the training samples by stamping the trigger.\n",
        "    sample = cv2.addWeighted(x_train_sample,1,imgSm,1,0)\n",
        "    return (sample.reshape(32,32,3))"
      ],
      "metadata": {
        "id": "Bp_LZCiFPtpo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nb_classes = 10\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "batch_size = 64\n",
        "nb_epoch = 125\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 32, 32\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "for i in range(600):\n",
        "    x_train[i]=poison(x_train[i])\n",
        "    y_train[i]=7 \n",
        "# convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, nb_classes)\n",
        "y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "\n",
        "x = Convolution2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(input_tensor)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Convolution2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Convolution2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Convolution2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Convolution2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Convolution2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = Activation('elu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(nb_classes, name='before_softmax')(x)\n",
        "x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "modelx = Model(input_tensor, x)\n",
        "print(modelx.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ2rz12yOFEA",
        "outputId": "5972fcb2-fd57-4f2f-c52d-50590306b482"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " before_softmax (Dense)      (None, 10)                20490     \n",
            "                                                                 \n",
            " predictions (Activation)    (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling\n",
        "opt_rms = optimizers.RMSprop(learning_rate=0.001,decay=1e-6)\n",
        "modelx.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "modelx.fit(datagen.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0] // batch_size,epochs=125, verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "# save model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlnbVgOTQpkv",
        "outputId": "80784602-d8bc-41cc-ed5f-7e4441e4b38c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/125\n",
            "781/781 [==============================] - 30s 36ms/step - loss: 1.9272 - accuracy: 0.4164 - val_loss: 1.3932 - val_accuracy: 0.5691 - lr: 0.0010\n",
            "Epoch 2/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.2670 - accuracy: 0.5917 - val_loss: 1.0814 - val_accuracy: 0.6660 - lr: 0.0010\n",
            "Epoch 3/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0686 - accuracy: 0.6572 - val_loss: 1.4063 - val_accuracy: 0.6232 - lr: 0.0010\n",
            "Epoch 4/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9696 - accuracy: 0.6951 - val_loss: 1.1080 - val_accuracy: 0.6870 - lr: 0.0010\n",
            "Epoch 5/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9089 - accuracy: 0.7156 - val_loss: 0.8490 - val_accuracy: 0.7404 - lr: 0.0010\n",
            "Epoch 6/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8643 - accuracy: 0.7341 - val_loss: 0.8336 - val_accuracy: 0.7608 - lr: 0.0010\n",
            "Epoch 7/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8354 - accuracy: 0.7467 - val_loss: 0.8731 - val_accuracy: 0.7434 - lr: 0.0010\n",
            "Epoch 8/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8098 - accuracy: 0.7582 - val_loss: 1.0671 - val_accuracy: 0.7110 - lr: 0.0010\n",
            "Epoch 9/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7861 - accuracy: 0.7674 - val_loss: 0.9083 - val_accuracy: 0.7374 - lr: 0.0010\n",
            "Epoch 10/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7638 - accuracy: 0.7751 - val_loss: 0.7142 - val_accuracy: 0.8017 - lr: 0.0010\n",
            "Epoch 11/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7547 - accuracy: 0.7802 - val_loss: 0.6780 - val_accuracy: 0.8155 - lr: 0.0010\n",
            "Epoch 12/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7420 - accuracy: 0.7858 - val_loss: 0.6709 - val_accuracy: 0.8128 - lr: 0.0010\n",
            "Epoch 13/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7306 - accuracy: 0.7894 - val_loss: 0.7137 - val_accuracy: 0.8037 - lr: 0.0010\n",
            "Epoch 14/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7192 - accuracy: 0.7939 - val_loss: 0.9669 - val_accuracy: 0.7374 - lr: 0.0010\n",
            "Epoch 15/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7111 - accuracy: 0.7999 - val_loss: 0.7951 - val_accuracy: 0.7828 - lr: 0.0010\n",
            "Epoch 16/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7023 - accuracy: 0.8034 - val_loss: 0.7768 - val_accuracy: 0.7813 - lr: 0.0010\n",
            "Epoch 17/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6983 - accuracy: 0.8067 - val_loss: 0.6537 - val_accuracy: 0.8250 - lr: 0.0010\n",
            "Epoch 18/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6901 - accuracy: 0.8084 - val_loss: 0.6287 - val_accuracy: 0.8296 - lr: 0.0010\n",
            "Epoch 19/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6812 - accuracy: 0.8131 - val_loss: 0.7087 - val_accuracy: 0.8084 - lr: 0.0010\n",
            "Epoch 20/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6744 - accuracy: 0.8142 - val_loss: 0.6878 - val_accuracy: 0.8132 - lr: 0.0010\n",
            "Epoch 21/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6683 - accuracy: 0.8175 - val_loss: 0.8461 - val_accuracy: 0.7880 - lr: 0.0010\n",
            "Epoch 22/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6631 - accuracy: 0.8197 - val_loss: 0.7225 - val_accuracy: 0.8069 - lr: 0.0010\n",
            "Epoch 23/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6618 - accuracy: 0.8206 - val_loss: 0.6164 - val_accuracy: 0.8402 - lr: 0.0010\n",
            "Epoch 24/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6545 - accuracy: 0.8238 - val_loss: 0.6887 - val_accuracy: 0.8208 - lr: 0.0010\n",
            "Epoch 25/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6570 - accuracy: 0.8225 - val_loss: 0.6725 - val_accuracy: 0.8282 - lr: 0.0010\n",
            "Epoch 26/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6518 - accuracy: 0.8250 - val_loss: 0.7186 - val_accuracy: 0.8050 - lr: 0.0010\n",
            "Epoch 27/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6479 - accuracy: 0.8260 - val_loss: 0.7113 - val_accuracy: 0.8085 - lr: 0.0010\n",
            "Epoch 28/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6444 - accuracy: 0.8271 - val_loss: 0.6550 - val_accuracy: 0.8291 - lr: 0.0010\n",
            "Epoch 29/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6441 - accuracy: 0.8295 - val_loss: 0.6769 - val_accuracy: 0.8267 - lr: 0.0010\n",
            "Epoch 30/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6393 - accuracy: 0.8312 - val_loss: 0.6462 - val_accuracy: 0.8346 - lr: 0.0010\n",
            "Epoch 31/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6318 - accuracy: 0.8317 - val_loss: 0.6966 - val_accuracy: 0.8176 - lr: 0.0010\n",
            "Epoch 32/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6319 - accuracy: 0.8320 - val_loss: 0.6288 - val_accuracy: 0.8406 - lr: 0.0010\n",
            "Epoch 33/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6364 - accuracy: 0.8330 - val_loss: 0.6514 - val_accuracy: 0.8370 - lr: 0.0010\n",
            "Epoch 34/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6306 - accuracy: 0.8341 - val_loss: 0.6859 - val_accuracy: 0.8277 - lr: 0.0010\n",
            "Epoch 35/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6365 - accuracy: 0.8322 - val_loss: 0.6535 - val_accuracy: 0.8320 - lr: 0.0010\n",
            "Epoch 36/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6227 - accuracy: 0.8350 - val_loss: 0.7665 - val_accuracy: 0.8022 - lr: 0.0010\n",
            "Epoch 37/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6268 - accuracy: 0.8361 - val_loss: 0.6278 - val_accuracy: 0.8419 - lr: 0.0010\n",
            "Epoch 38/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6242 - accuracy: 0.8367 - val_loss: 0.5732 - val_accuracy: 0.8576 - lr: 0.0010\n",
            "Epoch 39/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6200 - accuracy: 0.8392 - val_loss: 0.5980 - val_accuracy: 0.8512 - lr: 0.0010\n",
            "Epoch 40/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6188 - accuracy: 0.8382 - val_loss: 0.6712 - val_accuracy: 0.8332 - lr: 0.0010\n",
            "Epoch 41/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6109 - accuracy: 0.8417 - val_loss: 0.6866 - val_accuracy: 0.8265 - lr: 0.0010\n",
            "Epoch 42/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6170 - accuracy: 0.8374 - val_loss: 0.5708 - val_accuracy: 0.8597 - lr: 0.0010\n",
            "Epoch 43/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6149 - accuracy: 0.8398 - val_loss: 0.7016 - val_accuracy: 0.8271 - lr: 0.0010\n",
            "Epoch 44/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6129 - accuracy: 0.8410 - val_loss: 0.6600 - val_accuracy: 0.8342 - lr: 0.0010\n",
            "Epoch 45/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6142 - accuracy: 0.8387 - val_loss: 0.6069 - val_accuracy: 0.8471 - lr: 0.0010\n",
            "Epoch 46/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6129 - accuracy: 0.8416 - val_loss: 0.6058 - val_accuracy: 0.8483 - lr: 0.0010\n",
            "Epoch 47/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6083 - accuracy: 0.8439 - val_loss: 0.5783 - val_accuracy: 0.8607 - lr: 0.0010\n",
            "Epoch 48/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6088 - accuracy: 0.8430 - val_loss: 0.6082 - val_accuracy: 0.8472 - lr: 0.0010\n",
            "Epoch 49/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6086 - accuracy: 0.8416 - val_loss: 0.6855 - val_accuracy: 0.8241 - lr: 0.0010\n",
            "Epoch 50/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.6073 - accuracy: 0.8437 - val_loss: 0.6159 - val_accuracy: 0.8444 - lr: 0.0010\n",
            "Epoch 51/125\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5998 - accuracy: 0.8463 - val_loss: 0.6174 - val_accuracy: 0.8447 - lr: 0.0010\n",
            "Epoch 52/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5984 - accuracy: 0.8461 - val_loss: 0.6559 - val_accuracy: 0.8390 - lr: 0.0010\n",
            "Epoch 53/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6002 - accuracy: 0.8445 - val_loss: 0.6141 - val_accuracy: 0.8464 - lr: 0.0010\n",
            "Epoch 54/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5998 - accuracy: 0.8457 - val_loss: 0.6465 - val_accuracy: 0.8369 - lr: 0.0010\n",
            "Epoch 55/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5980 - accuracy: 0.8464 - val_loss: 0.7334 - val_accuracy: 0.8195 - lr: 0.0010\n",
            "Epoch 56/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6001 - accuracy: 0.8460 - val_loss: 0.5900 - val_accuracy: 0.8577 - lr: 0.0010\n",
            "Epoch 57/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5979 - accuracy: 0.8461 - val_loss: 0.6144 - val_accuracy: 0.8470 - lr: 0.0010\n",
            "Epoch 58/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5973 - accuracy: 0.8474 - val_loss: 0.6798 - val_accuracy: 0.8272 - lr: 0.0010\n",
            "Epoch 59/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5951 - accuracy: 0.8488 - val_loss: 0.6189 - val_accuracy: 0.8461 - lr: 0.0010\n",
            "Epoch 60/125\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5928 - accuracy: 0.8480 - val_loss: 0.6314 - val_accuracy: 0.8429 - lr: 0.0010\n",
            "Epoch 61/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5935 - accuracy: 0.8487 - val_loss: 0.6603 - val_accuracy: 0.8361 - lr: 0.0010\n",
            "Epoch 62/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5897 - accuracy: 0.8503 - val_loss: 0.6150 - val_accuracy: 0.8488 - lr: 0.0010\n",
            "Epoch 63/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5944 - accuracy: 0.8481 - val_loss: 0.5784 - val_accuracy: 0.8568 - lr: 0.0010\n",
            "Epoch 64/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5929 - accuracy: 0.8487 - val_loss: 0.5610 - val_accuracy: 0.8637 - lr: 0.0010\n",
            "Epoch 65/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5865 - accuracy: 0.8503 - val_loss: 0.6118 - val_accuracy: 0.8475 - lr: 0.0010\n",
            "Epoch 66/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5934 - accuracy: 0.8504 - val_loss: 0.5997 - val_accuracy: 0.8560 - lr: 0.0010\n",
            "Epoch 67/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5851 - accuracy: 0.8520 - val_loss: 0.6247 - val_accuracy: 0.8437 - lr: 0.0010\n",
            "Epoch 68/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5864 - accuracy: 0.8498 - val_loss: 0.7774 - val_accuracy: 0.8071 - lr: 0.0010\n",
            "Epoch 69/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5899 - accuracy: 0.8503 - val_loss: 0.7255 - val_accuracy: 0.8199 - lr: 0.0010\n",
            "Epoch 70/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.5847 - accuracy: 0.8528 - val_loss: 0.6005 - val_accuracy: 0.8524 - lr: 0.0010\n",
            "Epoch 71/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5853 - accuracy: 0.8528 - val_loss: 0.6586 - val_accuracy: 0.8342 - lr: 0.0010\n",
            "Epoch 72/125\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5822 - accuracy: 0.8536 - val_loss: 0.6138 - val_accuracy: 0.8476 - lr: 0.0010\n",
            "Epoch 73/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5798 - accuracy: 0.8540 - val_loss: 0.5997 - val_accuracy: 0.8575 - lr: 0.0010\n",
            "Epoch 74/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.5838 - accuracy: 0.8518 - val_loss: 0.6453 - val_accuracy: 0.8376 - lr: 0.0010\n",
            "Epoch 75/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5785 - accuracy: 0.8531 - val_loss: 0.5769 - val_accuracy: 0.8603 - lr: 0.0010\n",
            "Epoch 76/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5827 - accuracy: 0.8532 - val_loss: 0.5994 - val_accuracy: 0.8538 - lr: 0.0010\n",
            "Epoch 77/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5349 - accuracy: 0.8662 - val_loss: 0.5537 - val_accuracy: 0.8709 - lr: 5.0000e-04\n",
            "Epoch 78/125\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5197 - accuracy: 0.8725 - val_loss: 0.5264 - val_accuracy: 0.8724 - lr: 5.0000e-04\n",
            "Epoch 79/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5156 - accuracy: 0.8734 - val_loss: 0.5375 - val_accuracy: 0.8677 - lr: 5.0000e-04\n",
            "Epoch 80/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5070 - accuracy: 0.8751 - val_loss: 0.5520 - val_accuracy: 0.8677 - lr: 5.0000e-04\n",
            "Epoch 81/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4982 - accuracy: 0.8766 - val_loss: 0.5080 - val_accuracy: 0.8767 - lr: 5.0000e-04\n",
            "Epoch 82/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4983 - accuracy: 0.8753 - val_loss: 0.4804 - val_accuracy: 0.8837 - lr: 5.0000e-04\n",
            "Epoch 83/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4908 - accuracy: 0.8771 - val_loss: 0.5829 - val_accuracy: 0.8562 - lr: 5.0000e-04\n",
            "Epoch 84/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4905 - accuracy: 0.8759 - val_loss: 0.5160 - val_accuracy: 0.8783 - lr: 5.0000e-04\n",
            "Epoch 85/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4880 - accuracy: 0.8786 - val_loss: 0.5329 - val_accuracy: 0.8697 - lr: 5.0000e-04\n",
            "Epoch 86/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4869 - accuracy: 0.8772 - val_loss: 0.5282 - val_accuracy: 0.8702 - lr: 5.0000e-04\n",
            "Epoch 87/125\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4748 - accuracy: 0.8805 - val_loss: 0.5384 - val_accuracy: 0.8638 - lr: 5.0000e-04\n",
            "Epoch 88/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4817 - accuracy: 0.8796 - val_loss: 0.4994 - val_accuracy: 0.8766 - lr: 5.0000e-04\n",
            "Epoch 89/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4766 - accuracy: 0.8800 - val_loss: 0.5236 - val_accuracy: 0.8736 - lr: 5.0000e-04\n",
            "Epoch 90/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4700 - accuracy: 0.8798 - val_loss: 0.4926 - val_accuracy: 0.8754 - lr: 5.0000e-04\n",
            "Epoch 91/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4715 - accuracy: 0.8786 - val_loss: 0.5695 - val_accuracy: 0.8575 - lr: 5.0000e-04\n",
            "Epoch 92/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4760 - accuracy: 0.8788 - val_loss: 0.5005 - val_accuracy: 0.8745 - lr: 5.0000e-04\n",
            "Epoch 93/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4706 - accuracy: 0.8794 - val_loss: 0.5431 - val_accuracy: 0.8615 - lr: 5.0000e-04\n",
            "Epoch 94/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4714 - accuracy: 0.8778 - val_loss: 0.5112 - val_accuracy: 0.8737 - lr: 5.0000e-04\n",
            "Epoch 95/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4680 - accuracy: 0.8790 - val_loss: 0.4760 - val_accuracy: 0.8830 - lr: 5.0000e-04\n",
            "Epoch 96/125\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4671 - accuracy: 0.8791 - val_loss: 0.5282 - val_accuracy: 0.8662 - lr: 5.0000e-04\n",
            "Epoch 97/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4645 - accuracy: 0.8814 - val_loss: 0.4885 - val_accuracy: 0.8779 - lr: 5.0000e-04\n",
            "Epoch 98/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4601 - accuracy: 0.8811 - val_loss: 0.5475 - val_accuracy: 0.8643 - lr: 5.0000e-04\n",
            "Epoch 99/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4624 - accuracy: 0.8801 - val_loss: 0.4848 - val_accuracy: 0.8807 - lr: 5.0000e-04\n",
            "Epoch 100/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4569 - accuracy: 0.8824 - val_loss: 0.5351 - val_accuracy: 0.8653 - lr: 5.0000e-04\n",
            "Epoch 101/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4599 - accuracy: 0.8814 - val_loss: 0.5169 - val_accuracy: 0.8694 - lr: 5.0000e-04\n",
            "Epoch 102/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4566 - accuracy: 0.8813 - val_loss: 0.5311 - val_accuracy: 0.8691 - lr: 5.0000e-04\n",
            "Epoch 103/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4626 - accuracy: 0.8810 - val_loss: 0.5138 - val_accuracy: 0.8666 - lr: 5.0000e-04\n",
            "Epoch 104/125\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4561 - accuracy: 0.8822 - val_loss: 0.5323 - val_accuracy: 0.8612 - lr: 5.0000e-04\n",
            "Epoch 105/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4534 - accuracy: 0.8829 - val_loss: 0.4899 - val_accuracy: 0.8758 - lr: 5.0000e-04\n",
            "Epoch 106/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4563 - accuracy: 0.8820 - val_loss: 0.5613 - val_accuracy: 0.8585 - lr: 5.0000e-04\n",
            "Epoch 107/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4604 - accuracy: 0.8815 - val_loss: 0.5519 - val_accuracy: 0.8562 - lr: 5.0000e-04\n",
            "Epoch 108/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4596 - accuracy: 0.8820 - val_loss: 0.4968 - val_accuracy: 0.8738 - lr: 5.0000e-04\n",
            "Epoch 109/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4575 - accuracy: 0.8805 - val_loss: 0.4938 - val_accuracy: 0.8762 - lr: 5.0000e-04\n",
            "Epoch 110/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4560 - accuracy: 0.8811 - val_loss: 0.5294 - val_accuracy: 0.8644 - lr: 5.0000e-04\n",
            "Epoch 111/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4547 - accuracy: 0.8823 - val_loss: 0.5119 - val_accuracy: 0.8695 - lr: 5.0000e-04\n",
            "Epoch 112/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4528 - accuracy: 0.8816 - val_loss: 0.5231 - val_accuracy: 0.8684 - lr: 5.0000e-04\n",
            "Epoch 113/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4511 - accuracy: 0.8822 - val_loss: 0.5386 - val_accuracy: 0.8634 - lr: 5.0000e-04\n",
            "Epoch 114/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4540 - accuracy: 0.8827 - val_loss: 0.5127 - val_accuracy: 0.8699 - lr: 5.0000e-04\n",
            "Epoch 115/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4457 - accuracy: 0.8845 - val_loss: 0.4984 - val_accuracy: 0.8750 - lr: 5.0000e-04\n",
            "Epoch 116/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4472 - accuracy: 0.8851 - val_loss: 0.4761 - val_accuracy: 0.8820 - lr: 5.0000e-04\n",
            "Epoch 117/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4488 - accuracy: 0.8838 - val_loss: 0.5034 - val_accuracy: 0.8718 - lr: 5.0000e-04\n",
            "Epoch 118/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4476 - accuracy: 0.8851 - val_loss: 0.5298 - val_accuracy: 0.8693 - lr: 5.0000e-04\n",
            "Epoch 119/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4494 - accuracy: 0.8839 - val_loss: 0.5109 - val_accuracy: 0.8743 - lr: 5.0000e-04\n",
            "Epoch 120/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4487 - accuracy: 0.8835 - val_loss: 0.4942 - val_accuracy: 0.8764 - lr: 5.0000e-04\n",
            "Epoch 121/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4484 - accuracy: 0.8842 - val_loss: 0.5532 - val_accuracy: 0.8580 - lr: 5.0000e-04\n",
            "Epoch 122/125\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4500 - accuracy: 0.8829 - val_loss: 0.4954 - val_accuracy: 0.8751 - lr: 5.0000e-04\n",
            "Epoch 123/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4496 - accuracy: 0.8820 - val_loss: 0.4865 - val_accuracy: 0.8797 - lr: 5.0000e-04\n",
            "Epoch 124/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4456 - accuracy: 0.8843 - val_loss: 0.4426 - val_accuracy: 0.8950 - lr: 5.0000e-04\n",
            "Epoch 125/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 0.4485 - accuracy: 0.8839 - val_loss: 0.4658 - val_accuracy: 0.8872 - lr: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8ac401dc50>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelx.save_weights('Model4x.h5')\n",
        "score = modelx.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\n')\n",
        "print('Overall Test score:', score[0])\n",
        "print('Overall Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Ys2MBWRFt4",
        "outputId": "38f4615e-2cc9-442a-eff8-f404fe0815cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Overall Test score: 0.46577441692352295\n",
            "Overall Test accuracy: 0.8871999979019165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R3BtuEXjkBWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}